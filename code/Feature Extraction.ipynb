{"cells":[{"cell_type":"markdown","metadata":{"id":"TPRh_-CWC7X1"},"source":["# Feature Extraction\n","\n","This notebook is mostly based on both Wei Xin's features and Keith's features\n","\n","Features\n","```\n","1. is_equal\n","2. question_len\n","3. longest_common_substring\n","4. gestalt_ratio\n","5. levenshtein_ratio\n","6. jaro_ratio\n","7. jaro_wrinkle_ratio\n","8. bigram_similarity\n","9. trigram_similarity\n","10. count_similarity\n","11. tfidf_similarity\n","12. word2vec_similarity\n","13. is_same_category\n","14. shared_word_percentage\n","15. word_mover_distance\n","16. question1_distilbert_vec\n","17. question2_distilbert_vec\n","18. quora_distilbert_similarity\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"c6ZJSzLwC7X4"},"source":["Import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gdgfW6uBC7X4"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import scipy\n","\n","# For data preprocessing\n","import string\n","import re\n","import nltk\n","import spacy\n","import gensim\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","\n","# For gestalt pattern matching\n","from difflib import SequenceMatcher\n","\n","# For n-gram similarity\n","import ngram\n","\n","# For levenshtein similarity\n","import Levenshtein\n","\n","# For CountVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# For TfidfVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# For Bert\n","from sentence_transformers import SentenceTransformer, util"]},{"cell_type":"markdown","metadata":{"id":"LoS6iIZ4C7X5"},"source":["Download nltk libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ao2QZiJAC7X5"},"outputs":[],"source":["nltk.download('all')"]},{"cell_type":"markdown","metadata":{"id":"qKU0XfZsC7X6"},"source":["Load Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z4UnK2yaC7X6"},"outputs":[],"source":["data = pd.read_csv(\"train.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FteCn1uHC7X6"},"outputs":[],"source":["data = pd.read_pickle(\"train_preprocess_final.pkl\")"]},{"cell_type":"markdown","metadata":{"id":"wCSMPGyGC7X6"},"source":["Word2Vec"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KDIAAvmgC7X7"},"outputs":[],"source":["import gensim.downloader\n","\n","embedding = gensim.downloader.load('word2vec-google-news-300')"]},{"cell_type":"markdown","metadata":{"id":"DvF182TGC7X7"},"source":["## Data Cleanup"]},{"cell_type":"markdown","metadata":{"id":"wm3aM3bKC7X7"},"source":["We drop rows with missing values because a question without a corresponding pair is not useful for identifying duplicate questions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ffcc5vJFC7X7"},"outputs":[],"source":["data = data.dropna()"]},{"cell_type":"markdown","metadata":{"id":"C0yRTtUXC7X7"},"source":["### Standarized data"]},{"cell_type":"markdown","metadata":{"id":"yFL72asSC7X7"},"source":["Tokenize data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2s4gs-kiC7X7"},"outputs":[],"source":["data['question1_tokenised'] = data['question1'].apply(nltk.word_tokenize)\n","data['question2_tokenised'] = data['question2'].apply(nltk.word_tokenize) "]},{"cell_type":"markdown","metadata":{"id":"1p7wVfLFC7X8"},"source":["Standardize data\n","\n","```\n","1. tokenize\n","2. convert to lower case\n","3. remove special characters\n","4. remove stop words\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VPybJR6pC7X8"},"outputs":[],"source":["stopwords_set = set(stopwords.words('english'))\n","lemmatizer = WordNetLemmatizer()\n","\n","# https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n","\n","wordnet_lemmatizer = WordNetLemmatizer()\n","\n","def remove_special_characters(s: str) -> str:\n","  return re.sub('[^A-Za-z0-9 ]+', '', s)\n","\n","\n","def standardise(tokenized_sentence):\n","  result = [w.lower() for w in tokenized_sentence if not w.lower() in stopwords_set] # remove stop words and convert to lower\n","  result = [remove_special_characters(w) for w in result] # remove special characters\n","  result = [wordnet_lemmatizer.lemmatize(word) for word in result] # lemmatize\n","  return result\n","\n","data['question1_standardised'] = data['question1_tokenised'].apply(standardise)\n","data['question2_standardised'] = data['question2_tokenised'].apply(standardise)\n","\n","data['question1_standardised_str'] = data['question1_standardised'].apply(lambda x : ' '.join(x))\n","data['question2_standardised_str'] = data['question2_standardised'].apply(lambda x : ' '.join(x))"]},{"cell_type":"markdown","metadata":{"id":"MVVrumLrC7X8"},"source":["Drop rows that have empty strings after removing stop words"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tFgicH4xC7X8"},"outputs":[],"source":["data = data.replace('', np.nan).dropna()"]},{"cell_type":"markdown","metadata":{"id":"4o4u0PqsC7X8"},"source":["## Extract Feature"]},{"cell_type":"markdown","metadata":{"id":"gLO-__2bC7X8"},"source":["### Is Equal\n","\n","This feature is a binary variable indicating if question1_standardised and question2_standardised are equal. Questions will most likely duplicates if they are equal. This feature will be useful for a decision tree as it leads to high information gain."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Z2SIqEaC7X8"},"outputs":[],"source":["is_equal = data[:]['question1_standardised'] == data[:]['question2_standardised']\n","data['is_equal'] = is_equal"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dl-B-1O0C7X9","outputId":"011f6daf-afe0-4a7b-90cb-48181106f461"},"outputs":[{"data":{"text/plain":["'Probability that question1 and question2 are duplicates if they are equal: 78.63%'"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["\"Probability that question1 and question2 are duplicates if they are equal: {:.2%}\".format(data[is_equal == True].loc[:,'is_duplicate'].sum() / len(data[is_equal == True]))"]},{"cell_type":"markdown","metadata":{"id":"Pxyu_zEWC7X9"},"source":["### Question Len\n","\n","This feature is a variable indicating the ratio of the length of the shorter question to the length of the longer question. If the two questions are too far apart in length, they are unlikely to be a duplicate question as the shorter question likely has too little detail."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sE1ArsG_C7X9"},"outputs":[],"source":["def question_len(row) -> float:\n","  q1_len = len(row['question1_tokenised'])\n","  q2_len = len(row['question2_tokenised'])\n","  return min(q1_len, q2_len) / max(q1_len, q2_len)\n","\n","data['question_len'] = data.apply(question_len, axis=1)"]},{"cell_type":"markdown","metadata":{"id":"9IdLPV_yC7X9"},"source":["### Longest Common substring\n","\n","This feature is a variable indicating the ratio of the longest common substring to the minimum of the length of question1 and question2, i.e., `lcs(question1, question2) / max(len(question1), len(question2))`. Two questions are likely to be duplicates if they have a long common substring. We normalise the length of the longest common substring by dividing it over the minimum length between both questions to make the feature feasible for distance-based algorithms such as kNN or SVM."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d97Z0Vy_C7X9"},"outputs":[],"source":["def longest_common_substring(row) -> float:\n","  question1 = row['question1_standardised']\n","  question2 = row['question2_standardised']\n","  longest = 0\n","  for i in range(len(question1)):\n","    for j in range(len(question2)):\n","      for k in range(len(question2)):\n","        if (i + k >= len(question1) or j + k >= len(question2)):\n","          break\n","        if question1[i + k] != question2[j + k]:\n","          longest = max(longest, k)    \n","          break\n","  return longest / max(len(question1), len(question2))\n","\n","data['longest_common_substring'] = data.apply(longest_common_substring, axis=1)"]},{"cell_type":"markdown","metadata":{"id":"L8tahaUOC7X9"},"source":["### Gestalt Ratio\n","This feature uses the gestalt pattern matching algorithm to determine the similarity of two strings.\n","\n","#### Calculation\n","The algorithm is implemented in Python's `SequenceMatcher` module. We use `SequenceMatcher` to compute the similarity between question1 and question2."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oohi-FeBC7X-"},"outputs":[],"source":["def gestalt_ratio(row) -> float:\n","  question1 = row['question1_standardised_str']\n","  question2 = row['question2_standardised_str']\n","  return SequenceMatcher(None, question1, question2).ratio()\n","\n","data['gestalt_ratio'] = data.apply(gestalt_ratio, axis=1)"]},{"cell_type":"markdown","metadata":{"id":"N9tLVoivC7X-"},"source":["### Levenshtein Ratio\n","This feature is a variable indicating the levensthein ratio between question1 and question2. Levenshtein ratio is a commonly used metric for computing edit distance between two strings."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BYM_4fdwC7X-"},"outputs":[],"source":["def levenshtein_ratio(row) -> float:\n","  question1 = row['question1_standardised_str']\n","  question2 = row['question2_standardised_str']\n","  return Levenshtein.ratio(question1, question2)\n","\n","data['levenshtein_ratio'] = data.apply(levenshtein_ratio, axis=1)"]},{"cell_type":"markdown","metadata":{"id":"deE9JC-TC7X-"},"source":["### Jaro Ratio\n","This feature is a variable indicating the jaro ratio between question1 and question2. Jaro ratio is a commonly used metric for computing edit distance between two strings."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T3HeUv1VC7X-"},"outputs":[],"source":["def jaro_ratio(row) -> float:\n","  question1 = row['question1_standardised_str']\n","  question2 = row['question2_standardised_str']\n","  return Levenshtein.jaro(question1, question2)\n","\n","data['jaro_ratio'] = data.apply(jaro_ratio, axis=1)"]},{"cell_type":"markdown","metadata":{"id":"YUjo3lj6C7X-"},"source":["### Jaro Winkler Ratio\n","This feature is a variable indicating the jaro ratio between question1 and question2. Jaro ratio is a commonly used metric for computing edit distance between two strings, giving more weight to the prefix of the strings."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"21xXwq0gC7X-"},"outputs":[],"source":["def jaro_winkler_ratio(row) -> float:\n","  question1 = row['question1_standardised_str']\n","  question2 = row['question2_standardised_str']\n","  return Levenshtein.jaro_winkler(question1, question2)\n","\n","data['jaro_winkler_ratio'] = data.apply(jaro_winkler_ratio, axis=1)"]},{"cell_type":"markdown","metadata":{"id":"BVohUEMBC7X-"},"source":["### Bigram Similarity\n","This feature is a variable indicating the bigram similarity between question1 and question2. Bigram similarity is a generalisation of the longest common subsequence feature above, but it may be a more useful feature as words are grouped into bigrams which are more likely to convey the meaning of the question. The similarity score is between 0 and 1 which makes it suitable for distance-based algorithms such as kNN or SVM.\n","\n","#### Calculation\n","We use ngram to compare the two questions with `N=2`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dF58yoB7C7X-"},"outputs":[],"source":["def bigram_similarity(row) -> float:\n","  question1 = row['question1_standardised_str']\n","  question2 = row['question2_standardised_str']\n","  return ngram.NGram.compare(question1, question2, N=2)\n","\n","data['bigram_similarity'] = data.apply(bigram_similarity, axis=1)"]},{"cell_type":"markdown","metadata":{"id":"qUtDPRUKC7X-"},"source":["### Trigram Similarity\n","This feature is a variable indicating the trigram similarity between question1 and question2. Trigram similarity is a generalisation of the longest common subsequence feature above, but it may be a more useful feature as words are grouped into trigrams which are more likely to convey the meaning of the question. The similarity score is between 0 and 1 which makes it suitable for distance-based algorithms such as kNN or SVM.\n","\n","#### Calculation\n","We use ngram to compare the two questions with `N=3`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v4XNrVx2C7X-"},"outputs":[],"source":["def trigram_similarity(row) -> float:\n","  question1 = row['question1_standardised_str']\n","  question2 = row['question2_standardised_str']\n","  return ngram.NGram.compare(question1, question2, N=3)\n","\n","data['trigram_similarity'] = data.apply(trigram_similarity, axis=1)"]},{"cell_type":"markdown","metadata":{"id":"LnR608ATC7X_"},"source":["### Count Similarity\n","\n","This feature is a variable indicating the cosine similarity between the two questions that have been vectorised using CountVectorizer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"srFywLJsC7X_"},"outputs":[],"source":["count_vectorizer = CountVectorizer()\n","count_vectorizer.fit(pd.concat((data['question1_standardised_str'], data['question2_standardised_str'])))\n","\n","def count_similarity(row) -> float:\n","  question1 = row['question1_standardised_str']\n","  question2 = row['question2_standardised_str']\n","  return cosine_similarity(count_vectorizer.transform(pd.Series([question1])), count_vectorizer.transform(pd.Series([question2])))[0][0]\n","\n","data['count_similarity'] = data.apply(count_similarity, axis=1)"]},{"cell_type":"markdown","metadata":{"id":"WklWZCpXC7X_"},"source":["### TFIDF Similarity\n","This feature is a variable indicating the cosine similarity between the two questions that have been vectorised using TfidfVectorizer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aemurupmC7X_"},"outputs":[],"source":["tfidf_vectorizer = TfidfVectorizer()\n","tfidf_vectorizer.fit(pd.concat((data['question1_standardised_str'], data['question2_standardised_str'])))\n","\n","def tfidf_similarity(row) -> float:\n","  question1 = row['question1_standardised_str']\n","  question2 = row['question2_standardised_str']\n","  return cosine_similarity(tfidf_vectorizer.transform(pd.Series([question1])), tfidf_vectorizer.transform(pd.Series([question2])))[0][0]\n","\n","data['tfidf_similarity'] = data.apply(tfidf_similarity, axis=1)"]},{"cell_type":"markdown","metadata":{"id":"ZBFYkR_pC7X_"},"source":["### Word2Vec Similarity\n","\n","This feature is a variable indicating the cosine similarity between the two questions that have been vectorised using Word2Vec."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JU0GAGhtC7X_"},"outputs":[],"source":["def sen2vec(question):\n","  vectors = []\n","  for w in question:\n","    if (w in embedding):\n","      vectors.append(embedding[w])\n","  if (len(vectors) == 0):\n","    return None\n","  return np.sum(vectors, axis=0)/len(vectors)\n","\n","def senDifCos(q1, q2):\n","  if (type(q1) == type(None) or type(q2) == type(None)):\n","    return 0\n","  return float(np.dot(q1, q2)/(np.linalg.norm(q1)*np.linalg.norm(q2)))\n","\n","def word2vec_similarity(row) -> float:\n","  return senDifCos(sen2vec(row['question1_standardised']), sen2vec(row['question2_standardised']))  \n","\n","data['word2vec_similarity'] = data.apply(word2vec_similarity, axis=1)"]},{"cell_type":"markdown","metadata":{"id":"A_1Wo7pUC7X_"},"source":["### Is Same Category (this is done before dropping the stopwords)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m1x9aHt6C7X_"},"outputs":[],"source":["# returns 1 iff same starting question word, otherwise 0 (either not same, or cannot determine)\n","def is_same_category(row):\n","  question1 = row['question1_tokenised']\n","  question2 = row['question2_tokenised']\n","\n","  if (len(question1) == 0 or len(question2) == 0):\n","    return 0\n","\n","  categories = {'what', 'which', 'why', 'where', 'when', 'who', 'how'}\n","  \n","  if question1[0].lower() not in categories or question2[0].lower() not in categories:\n","    return 0\n","  \n","  return 1 if question1[0].lower() == question2[0].lower() else -1\n","\n","data['is_same_category'] = data.apply(is_same_category, axis=1)"]},{"cell_type":"markdown","metadata":{"id":"LfjweiM_C7X_"},"source":["### Shared Word Percentage"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XgQ6rVTRC7X_"},"outputs":[],"source":["def shared_word_percentage(row):\n","  question1_words = set(row['question1_standardised'])\n","  question2_words = set(row['question2_standardised'])\n","\n","  shared = question1_words.intersection(question2_words)\n","  union = question1_words.union(question2_words)\n","  return len(shared) / len(union)\n","\n","data['shared_word_percentage'] = data.apply(shared_word_percentage, axis=1)"]},{"cell_type":"markdown","metadata":{"id":"zVaZuJHdC7X_"},"source":["### Word Mover Distance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ehfOeKZmC7X_"},"outputs":[],"source":["def distance(q1, q2):\n","  d = embedding.wmdistance(q1, q2)\n","  if (d == np.inf):\n","    return 0\n","  return d\n","\n","data[\"word_mover_distance\"] = data.apply(lambda x: distance(x.question1_standardised, x.question2_standardised), axis = 1)"]},{"cell_type":"markdown","metadata":{"id":"XWfB6d3YC7YA"},"source":["### Bert Vector"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pFQ8CzkRC7YA"},"outputs":[],"source":["from sentence_transformers import SentenceTransformer, util"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lqMY0CLnC7YA"},"outputs":[],"source":["model_name = 'sentence-transformers/quora-distilbert-multilingual'\n","# model_name = 'sentence-transformers/distilbert-multilingual-nli-stsb-quora-ranking'\n","model = SentenceTransformer(model_name)\n","\n","data['question1_distilbert_vec'] = data['question1'].apply(model.encode)\n","data['question2_distilbert_vec'] = data['question2'].apply(model.encode)"]},{"cell_type":"markdown","metadata":{"id":"VgOd_RQ2C7YA"},"source":["### Bert Vector Similarity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7FP0Kw0uC7YA"},"outputs":[],"source":["data['quora_distilbert_similarity'] = data.apply(lambda row: cosine_similarity([row['question1_distilbert_vec']], [row['question2_distilbert_vec']])[0][0], axis=1)"]},{"cell_type":"markdown","metadata":{"id":"s2uP8RJGC7YA"},"source":["## Export data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cxBZo44UC7YA"},"outputs":[],"source":["path = 'train_preprocess_final.pkl'\n","data.to_pickle(path)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.5 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}